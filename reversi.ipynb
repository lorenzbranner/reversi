{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(0)\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiton Reversi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_map_file(filepath, max_players):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        lines = [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "    player_count = int(lines[0])\n",
    "    if player_count > max_players:\n",
    "        raise ValueError(f\"Maximal {max_players} Spieler erlaubt, aber {player_count} gefunden.\")\n",
    "\n",
    "    height, width = map(int, lines[3].split())\n",
    "    raw_map_lines = lines[4:]\n",
    "    \n",
    "    if len(raw_map_lines) != height:\n",
    "        raise ValueError(f\"Erwartet {height} Zeilen, aber {len(raw_map_lines)} Zeilen in Map gefunden.\")\n",
    "\n",
    "    flat_map = []\n",
    "    for line in raw_map_lines:\n",
    "        tokens = line.split()\n",
    "        if len(tokens) != width:\n",
    "            raise ValueError(f\"Erwartet {width} Spalten, aber Zeile hat {len(tokens)} Werte: {line}\")\n",
    "        for token in tokens:\n",
    "            if token == '-':\n",
    "                flat_map.append(5)\n",
    "            else:\n",
    "                val = int(token)\n",
    "                if not (0 <= val <= player_count):\n",
    "                    raise ValueError(f\"UngÃ¼ltiger Spielsteinwert: {val}\")\n",
    "                flat_map.append(val)\n",
    "\n",
    "    board = np.array(flat_map, dtype=np.uint8).reshape((height, width))\n",
    "    padded_board = np.full((15, 15), 5, dtype=np.uint8)\n",
    "    padded_board[:height, :width] = board\n",
    "    return padded_board\n",
    "\n",
    "class Reversi:\n",
    "    DIRECTIONS = [(-1, 0), (-1, 1), (0, 1), (1, 1),\n",
    "                  (1, 0), (1, -1), (0, -1), (-1, -1)]\n",
    "    def __init__(self):\n",
    "        self.width = 15\n",
    "        self.height = 15\n",
    "        self.action_size = self.width * self.height\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Reversi\"\n",
    "    \n",
    "    def get_initial_board(self, filepath, max_players):\n",
    "        return parse_map_file(filepath, max_players)\n",
    "\n",
    "    def get_next_board(self, board, move, player):\n",
    "        board = board.copy()\n",
    "        x, y = move[0], move[1]\n",
    "        board[y, x] = player\n",
    "        for dx, dy in self.DIRECTIONS:\n",
    "            path = []\n",
    "            cx, cy = x + dx, y + dy\n",
    "            while 0 <= cx < board.shape[1] and 0 <= cy < board.shape[0]:\n",
    "                val = board[cy, cx]\n",
    "                if val == 0 or val == 5:\n",
    "                    break\n",
    "                if val == player:\n",
    "                    for px, py in path:\n",
    "                        board[py, px] = player\n",
    "                    break\n",
    "                path.append((cx, cy))\n",
    "                cx += dx\n",
    "                cy += dy\n",
    "        return board\n",
    "    \n",
    "    def get_valid_moves(self, board, player):\n",
    "        valid_moves = []\n",
    "        for y in range(self.height):\n",
    "            for x in range(self.width):\n",
    "                if self.is_valid_move(board, x, y, player):\n",
    "                    valid_moves.append((x, y))\n",
    "        return valid_moves\n",
    "    \n",
    "    def is_valid_move(self, board, x, y, player):\n",
    "        if not (0 <= x < self.width and 0 <= y < self.height):\n",
    "            return False\n",
    "        if board[y, x] != 0:\n",
    "            return False\n",
    "\n",
    "        for dx, dy in self.DIRECTIONS:\n",
    "            if self.validate_direction(board, x, y, dx, dy, player):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def validate_direction(self, board, x, y, dx, dy, player):\n",
    "        enemy_range = set(range(1, 10)) - {player}\n",
    "        x += dx\n",
    "        y += dy\n",
    "        found_enemy = False\n",
    "\n",
    "        while 0 <= x < self.width and 0 <= y < self.height:\n",
    "            val = board[y, x]\n",
    "            if val in enemy_range:\n",
    "                found_enemy = True\n",
    "            elif val == player:\n",
    "                return found_enemy\n",
    "            else:\n",
    "                return False\n",
    "            x += dx\n",
    "            y += dy\n",
    "        return False\n",
    "        \n",
    "    def valid_move_player(self, board, player):\n",
    "        for y in range(self.height):\n",
    "            for x in range(self.width):\n",
    "                if self.is_valid_move(board, x, y, player):\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    def game_over(self, board, num_player) -> bool:\n",
    "        for player in range(1, num_player + 1):\n",
    "            if self.valid_move_player(board, player):\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def get_next_player(self, player, num_players):\n",
    "        if player == num_players:\n",
    "            return 1\n",
    "        return player + 1\n",
    "    \n",
    "    def get_values(self, board, num_players):\n",
    "        player_counts = np.array([np.count_nonzero(board == p) for p in range(1, num_players + 1)])\n",
    "\n",
    "        active_indices = np.nonzero(player_counts)[0]\n",
    "        if len(active_indices) == 0:\n",
    "            return np.zeros(num_players, dtype=np.float32)\n",
    "\n",
    "        counts = player_counts[active_indices]\n",
    "        sorted_indices = active_indices[np.argsort(-counts)]\n",
    "        rank_points = np.array([25, 11, 5, 2])[:len(sorted_indices)]\n",
    "\n",
    "        min_p, max_p = rank_points.min(), rank_points.max()\n",
    "        if min_p == max_p:\n",
    "            scaled = np.zeros_like(rank_points, dtype=np.float32)\n",
    "        else:\n",
    "            scaled = (rank_points - min_p) / (max_p - min_p) * 2 - 1\n",
    "\n",
    "        result = np.zeros(num_players, dtype=np.float32)\n",
    "        for idx, val in zip(sorted_indices, scaled):\n",
    "            result[idx] = val\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Reversi (Playing with yourself)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reversi = Reversi()\n",
    "player = 1\n",
    "num_players = 2\n",
    "\n",
    "board = reversi.get_initial_board(\"maps/simulate.map\", 4)\n",
    "\n",
    "while True:\n",
    "    print(board)\n",
    "    valid_moves = reversi.get_valid_moves(board, player)\n",
    "    print(valid_moves)\n",
    "    move = tuple(map(int, input(f\"{player}: \").split()))\n",
    "\n",
    "    if move not in valid_moves:\n",
    "        print(\"move not valid\")\n",
    "        continue\n",
    "\n",
    "    board = reversi.get_next_board(board, move, player)\n",
    "\n",
    "    if reversi.game_over(board, num_players):\n",
    "        print(board)\n",
    "        values = reversi.get_values(board, num_players)\n",
    "        print(values)\n",
    "        break\n",
    "\n",
    "    player = reversi.get_next_player(player, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MCTS for Reversi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, game, C, board, num_players, current_player, parent=None, move_taken=None):\n",
    "        self.game = game\n",
    "        self.C = C\n",
    "        self.board = board\n",
    "        self.num_players = num_players\n",
    "        self.current_player = current_player\n",
    "        self.parent = parent\n",
    "        self.move_taken = move_taken\n",
    "        \n",
    "        self.children = []\n",
    "        self.expandable_moves = game.get_valid_moves(board, current_player)\n",
    "        \n",
    "        self.visit_count = 0\n",
    "        self.values = np.zeros(num_players)\n",
    "        \n",
    "    def is_fully_expanded(self):\n",
    "        return np.sum(self.expandable_moves) == 0 and len(self.children) > 0\n",
    "    \n",
    "    def select(self):\n",
    "        best_child = None\n",
    "        best_ucb = -np.inf\n",
    "        \n",
    "        for child in self.children:\n",
    "            ucb = self.get_ucb(child)\n",
    "            if ucb > best_ucb:\n",
    "                best_child = child\n",
    "                best_ucb = ucb\n",
    "                \n",
    "        return best_child\n",
    "    \n",
    "    def get_ucb(self, child):\n",
    "        q_value = 1 - ((child.values[child.current_player-1] / child.visit_count) + 1) / 2\n",
    "        return q_value + self.C * math.sqrt(math.log(self.visit_count) / child.visit_count)\n",
    "    \n",
    "    def expand(self):\n",
    "        move = random.choice(self.expandable_moves)\n",
    "        self.expandable_moves.remove(move)\n",
    "        \n",
    "        child_board = self.board.copy()\n",
    "        child_board = self.game.get_next_board(child_board, move, self.current_player)\n",
    "\n",
    "        child_player = self.game.get_next_player(self.current_player, self.num_players)\n",
    "        \n",
    "        child = Node(self.game, self.C, child_board, self.num_players, child_player, self, move)\n",
    "        self.children.append(child)\n",
    "        return child\n",
    "    \n",
    "    def simulate(self):\n",
    "        rollout_board = self.board.copy()\n",
    "        rollout_player = self.current_player\n",
    "        \n",
    "        while (True):\n",
    "            if self.game.game_over(rollout_board, self.num_players):\n",
    "                break\n",
    "\n",
    "            valid_moves = self.game.get_valid_moves(rollout_board, rollout_player)\n",
    "\n",
    "            if valid_moves:\n",
    "                random_move = random.choice(valid_moves)\n",
    "                rollout_board = self.game.get_next_board(rollout_board, random_move, rollout_player)\n",
    "\n",
    "            rollout_player = self.game.get_next_player(rollout_player, self.num_players)\n",
    "        \n",
    "        return self.game.get_values(rollout_board, self.num_players)\n",
    "            \n",
    "    def backpropagate(self, values):\n",
    "        self.visit_count += 1\n",
    "        self.values += values\n",
    "        \n",
    "        if self.parent is not None:\n",
    "            self.parent.backpropagate(values)  \n",
    "\n",
    "\n",
    "class MCTS:\n",
    "    def __init__(self, game, C):\n",
    "        self.game = game\n",
    "        self.C = C\n",
    "        \n",
    "    def search(self, board, num_players, player, num_searches):\n",
    "        root = Node(self.game, self.C, board, num_players, player)\n",
    "        \n",
    "        for search in range(num_searches):\n",
    "            node = root\n",
    "            \n",
    "            while node.is_fully_expanded():\n",
    "                node = node.select()\n",
    "                \n",
    "            game_finished = self.game.game_over(node.board, node.num_players)\n",
    "            values = None\n",
    "            \n",
    "            if not game_finished:\n",
    "                node = node.expand()\n",
    "                values = node.simulate()\n",
    "            else:\n",
    "                values = self.game.get_values(node.board, node.num_players)\n",
    "                \n",
    "            node.backpropagate(values)            \n",
    "            \n",
    "        action_probs = np.zeros((self.game.height, self.game.width))\n",
    "        for child in root.children:\n",
    "            action_probs[child.move_taken[1], child.move_taken[0]] = child.visit_count\n",
    "        action_probs /= np.sum(action_probs)\n",
    "        return action_probs\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Playing against MCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 0 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 1 2 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 2 1 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 0 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 0 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]]\n",
      "Move taken:  (np.int64(4), np.int64(2))\n",
      "[[0 0 0 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 0 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 1 1 1 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 2 1 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 0 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 0 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]]\n",
      "[(2, 1), (4, 1), (4, 3)]\n",
      "Move taken:  (2, 1)\n",
      "[[0 0 0 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 2 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 2 1 1 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 2 1 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 0 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 0 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]]\n",
      "Move taken:  (np.int64(1), np.int64(0))\n",
      "[[0 1 0 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 1 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 2 1 1 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 2 1 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 0 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 0 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]]\n",
      "[(2, 0), (4, 1), (5, 2), (4, 3), (4, 4)]\n",
      "Move taken:  (2, 0)\n",
      "[[0 1 2 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 2 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 2 1 1 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 2 1 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 0 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 0 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]]\n",
      "Move taken:  (np.int64(3), np.int64(0))\n",
      "[[0 1 1 1 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 2 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 2 1 1 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 2 1 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 0 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 0 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]]\n",
      "[(4, 1), (5, 2), (4, 3), (4, 4)]\n",
      "Move taken:  (4, 1)\n",
      "[[0 1 1 1 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 2 0 2 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 2 2 1 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 2 1 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 0 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 0 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]]\n",
      "Move taken:  (np.int64(5), np.int64(2))\n",
      "[[0 1 1 1 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 2 0 1 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 2 2 1 1 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 2 1 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 0 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 0 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]]\n",
      "[(5, 0), (4, 3), (3, 4), (4, 4)]\n",
      "Move taken:  (5, 0)\n",
      "[[0 1 1 1 0 2 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 2 0 2 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 2 2 1 1 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 2 1 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 0 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 0 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]]\n",
      "Move taken:  (np.int64(1), np.int64(1))\n",
      "[[0 1 1 1 0 2 5 5 5 5 5 5 5 5 5]\n",
      " [0 1 2 0 2 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 1 2 1 1 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 2 1 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 0 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 0 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]]\n",
      "[(0, 1), (1, 2), (4, 3), (3, 4)]\n",
      "Move taken:  (0, 1)\n",
      "[[0 1 1 1 0 2 5 5 5 5 5 5 5 5 5]\n",
      " [2 2 2 0 2 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 1 2 1 1 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 2 1 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 0 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 0 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]]\n",
      "Move taken:  (np.int64(1), np.int64(2))\n",
      "[[0 1 1 1 0 2 5 5 5 5 5 5 5 5 5]\n",
      " [2 1 1 0 2 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 1 1 2 1 1 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 2 1 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 0 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 0 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]]\n",
      "[(3, 1), (0, 2), (4, 3), (3, 4)]\n",
      "Move taken:  (3, 1)\n",
      "[[0 1 1 1 0 2 5 5 5 5 5 5 5 5 5]\n",
      " [2 2 2 2 2 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 1 1 2 1 1 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 2 1 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 0 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 0 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]]\n",
      "Move taken:  (np.int64(0), np.int64(0))\n",
      "[[1 1 1 1 0 2 5 5 5 5 5 5 5 5 5]\n",
      " [2 1 2 2 2 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 1 1 2 1 1 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 2 1 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 0 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 0 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]]\n",
      "[(0, 2), (0, 3), (1, 3), (4, 3), (5, 3), (3, 4)]\n",
      "Move taken:  (0, 2)\n",
      "[[1 1 1 1 0 2 5 5 5 5 5 5 5 5 5]\n",
      " [2 1 2 2 2 0 5 5 5 5 5 5 5 5 5]\n",
      " [2 2 2 2 1 1 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 2 1 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 0 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 0 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]]\n",
      "Move taken:  (np.int64(4), np.int64(0))\n",
      "[[1 1 1 1 1 2 5 5 5 5 5 5 5 5 5]\n",
      " [2 1 2 2 1 0 5 5 5 5 5 5 5 5 5]\n",
      " [2 2 2 2 1 1 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 2 1 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 0 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 0 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]]\n",
      "[(5, 1), (4, 3), (5, 3), (3, 4), (4, 4)]\n",
      "Move taken:  (5, 1)\n",
      "[[1 1 1 1 1 2 5 5 5 5 5 5 5 5 5]\n",
      " [2 1 2 2 2 2 5 5 5 5 5 5 5 5 5]\n",
      " [2 2 2 2 1 1 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 2 1 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 0 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 0 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]]\n",
      "Move taken:  (np.int64(1), np.int64(3))\n",
      "[[1 1 1 1 1 2 5 5 5 5 5 5 5 5 5]\n",
      " [2 1 2 1 2 2 5 5 5 5 5 5 5 5 5]\n",
      " [2 1 1 2 1 1 5 5 5 5 5 5 5 5 5]\n",
      " [0 1 1 1 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 0 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 0 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]]\n",
      "[(0, 3), (4, 3), (5, 3), (1, 4), (2, 4), (3, 4)]\n",
      "Move taken:  (0, 3)\n",
      "[[1 1 1 1 1 2 5 5 5 5 5 5 5 5 5]\n",
      " [2 1 2 1 2 2 5 5 5 5 5 5 5 5 5]\n",
      " [2 2 1 2 1 1 5 5 5 5 5 5 5 5 5]\n",
      " [2 1 1 1 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 0 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 0 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]]\n",
      "Move taken:  (np.int64(0), np.int64(4))\n",
      "[[1 1 1 1 1 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 1 2 1 2 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 2 1 2 1 1 5 5 5 5 5 5 5 5 5]\n",
      " [1 1 1 1 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [1 0 0 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 0 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]]\n",
      "[(4, 3), (5, 3), (1, 4), (2, 4), (3, 4)]\n",
      "Move taken:  (4, 3)\n",
      "[[1 1 1 1 1 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 1 2 1 2 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 2 1 2 2 1 5 5 5 5 5 5 5 5 5]\n",
      " [1 1 1 1 2 0 5 5 5 5 5 5 5 5 5]\n",
      " [1 0 0 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 0 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]]\n",
      "Move taken:  (np.int64(3), np.int64(4))\n",
      "[[1 1 1 1 1 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 1 2 1 2 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 2 1 2 2 1 5 5 5 5 5 5 5 5 5]\n",
      " [1 1 1 1 1 0 5 5 5 5 5 5 5 5 5]\n",
      " [1 0 0 1 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 0 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]]\n",
      "[(5, 3), (1, 4), (2, 4), (4, 4), (5, 4), (3, 5), (4, 5)]\n",
      "Move taken:  (5, 3)\n",
      "[[1 1 1 1 1 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 1 2 1 2 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 2 1 2 2 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 1 1 1 1 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 0 0 1 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 0 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]]\n",
      "[[1 1 1 1 1 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 1 2 1 2 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 2 1 2 2 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 1 1 1 1 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 0 0 1 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 0 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]]\n",
      "[(1, 4), (2, 4), (4, 4), (5, 4), (2, 5), (3, 5), (4, 5)]\n",
      "Move taken:  (1, 4)\n",
      "[[1 1 1 1 1 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 1 2 1 2 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 2 1 2 2 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 2 2 1 1 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 2 0 1 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 0 0 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]]\n",
      "Move taken:  (np.int64(1), np.int64(5))\n",
      "[[1 1 1 1 1 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 1 2 1 2 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 1 1 2 2 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 1 2 1 1 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 1 0 1 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 1 0 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]]\n",
      "[(2, 4), (4, 4), (5, 4), (0, 5), (2, 5), (3, 5), (4, 5)]\n",
      "Move taken:  (2, 4)\n",
      "[[1 1 1 1 1 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 1 2 1 2 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 1 1 2 2 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 1 2 2 1 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 1 2 1 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 1 0 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]]\n",
      "Move taken:  (np.int64(2), np.int64(5))\n",
      "[[1 1 1 1 1 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 1 2 1 2 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 1 1 2 2 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 1 1 2 1 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 1 1 1 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 1 1 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]]\n",
      "[(4, 4), (5, 4), (0, 5), (3, 5)]\n",
      "Move taken:  (4, 4)\n",
      "[[1 1 1 1 1 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 1 2 1 2 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 1 1 2 2 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 1 1 2 2 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 1 1 1 2 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 1 1 0 0 0 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]]\n",
      "Move taken:  (np.int64(4), np.int64(5))\n",
      "[[1 1 1 1 1 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 1 2 1 1 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 1 1 2 1 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 1 1 2 1 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 1 1 1 1 0 5 5 5 5 5 5 5 5 5]\n",
      " [0 1 1 0 1 0 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]]\n",
      "[(5, 4), (0, 5), (3, 5), (5, 5)]\n",
      "Move taken:  (5, 4)\n",
      "[[1 1 1 1 1 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 1 2 1 1 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 1 1 2 1 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 1 1 2 2 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 1 1 1 1 2 5 5 5 5 5 5 5 5 5]\n",
      " [0 1 1 0 1 0 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]]\n",
      "[[1 1 1 1 1 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 1 2 1 1 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 1 1 2 1 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 1 1 2 2 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 1 1 1 1 2 5 5 5 5 5 5 5 5 5]\n",
      " [0 1 1 0 1 0 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]]\n",
      "[(0, 5), (3, 5), (5, 5)]\n",
      "Move taken:  (0, 5)\n",
      "[[1 1 1 1 1 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 1 2 1 1 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 1 1 2 1 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 1 2 2 2 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 2 1 1 1 2 5 5 5 5 5 5 5 5 5]\n",
      " [2 1 1 0 1 0 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]]\n",
      "[[1 1 1 1 1 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 1 2 1 1 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 1 1 2 1 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 1 2 2 2 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 2 1 1 1 2 5 5 5 5 5 5 5 5 5]\n",
      " [2 1 1 0 1 0 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]]\n",
      "[(3, 5), (5, 5)]\n",
      "Move taken:  (3, 5)\n",
      "[[1 1 1 1 1 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 1 2 1 1 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 1 1 2 1 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 1 2 2 2 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 2 1 2 2 2 5 5 5 5 5 5 5 5 5]\n",
      " [2 2 2 2 1 0 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]]\n",
      "Move taken:  (np.int64(5), np.int64(5))\n",
      "[[1 1 1 1 1 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 1 2 1 1 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 1 1 2 1 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 1 2 1 2 2 5 5 5 5 5 5 5 5 5]\n",
      " [1 2 1 2 1 2 5 5 5 5 5 5 5 5 5]\n",
      " [2 2 2 2 1 1 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      " [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]]\n",
      "[ 1. -1.]\n"
     ]
    }
   ],
   "source": [
    "reversi = Reversi()\n",
    "player = 1\n",
    "num_players = 2\n",
    "\n",
    "mcts = MCTS(reversi, 1.41)\n",
    "\n",
    "board = reversi.get_initial_board(\"maps/small.map\", 2)\n",
    "\n",
    "while True:\n",
    "    print(board)\n",
    "\n",
    "    move = None\n",
    "    \n",
    "    if player == 2:\n",
    "        valid_moves = reversi.get_valid_moves(board, player)\n",
    "        print(valid_moves)\n",
    "        move = tuple(map(int, input(f\"{player}: \").split()))\n",
    "\n",
    "        if move not in valid_moves:\n",
    "            print(\"move not valid\")\n",
    "            continue\n",
    "            \n",
    "    else:\n",
    "        if reversi.get_valid_moves(board, player):\n",
    "            mcts_probs = mcts.search(board, num_players, player, 10)\n",
    "            move = tuple(reversed(np.unravel_index(np.argmax(mcts_probs), mcts_probs.shape)))\n",
    "\n",
    "    if move:    \n",
    "        print(\"Move taken: \", move)\n",
    "        board = reversi.get_next_board(board, move, player)\n",
    "            \n",
    "        if reversi.game_over(board, num_players):\n",
    "            print(board)\n",
    "            values = reversi.get_values(board, num_players)\n",
    "            print(values)\n",
    "            break\n",
    "\n",
    "    player = reversi.get_next_player(player, num_players)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, game, num_resBlocks, num_hidden, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.startBlock = nn.Sequential(\n",
    "            nn.Conv2d(3, num_hidden, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(num_hidden),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.backBone = nn.ModuleList(\n",
    "            [ResBlock(num_hidden) for i in range(num_resBlocks)]\n",
    "        )\n",
    "\n",
    "        self.policyHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * game.height * game.width, game.action_size)\n",
    "        )\n",
    "\n",
    "        self.valueHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 3, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3 * game.height * game.width, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.startBlock(x)\n",
    "        for resBlock in self.backBone:\n",
    "            x = resBlock(x)\n",
    "        policy = self.policyHead(x)\n",
    "        value = self.valueHead(x)\n",
    "        return policy, value\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, num_hidden):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_hidden)\n",
    "        self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_hidden)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x += residual\n",
    "        x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MCTS works, from now on not finished"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test TicTacToe (Testing if Suggestions are good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  1.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0. -1.  0.]]\n",
      "[[[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 1. 0.]]\n",
      "\n",
      " [[1. 1. 0.]\n",
      "  [1. 1. 1.]\n",
      "  [1. 0. 1.]]\n",
      "\n",
      " [[0. 0. 1.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]]\n",
      "0.15747429430484772 [0.06009828 0.22448952 0.1137168  0.11022214 0.10687571 0.1522677\n",
      " 0.07634947 0.05143549 0.10454486]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tictactoe = TicTacToe()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "state = tictactoe.get_initial_state()\n",
    "state = tictactoe.get_next_state(state, 2, 1)\n",
    "state = tictactoe.get_next_state(state, 7, -1)\n",
    "\n",
    "print(state)\n",
    "\n",
    "encoded_state = tictactoe.get_encoded_state(state)\n",
    "print(encoded_state)\n",
    "\n",
    "tensor_state = torch.tensor(encoded_state, device=device).unsqueeze(0)\n",
    "\n",
    "model = ResNet(tictactoe, 4, 64, device=device)\n",
    "model.load_state_dict(torch.load(\"model_2.pt\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "policy, value = model(tensor_state)\n",
    "value = value.item()\n",
    "policy = torch.softmax(policy, axis=1).squeeze(0).detach().numpy()\n",
    "\n",
    "print(value, policy)\n",
    "\n",
    "plt.bar(range(tictactoe.action_size), policy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition MCTS and Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, game, args, state, parent=None, action_taken=None, prior=0, visit_count=0):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.action_taken = action_taken\n",
    "        self.prior = prior\n",
    "\n",
    "        self.children = []\n",
    "        self.expandable_moves = game.get_valid_moves(state)\n",
    "\n",
    "        self.visit_count = visit_count\n",
    "        self.value_sum = 0\n",
    "\n",
    "    def is_fully_expanded(self):\n",
    "        return len(self.children) > 0\n",
    "    \n",
    "    def select(self):\n",
    "        best_child = None\n",
    "        best_ucb = -np.inf\n",
    "        for child in self.children:\n",
    "            ucb = self.get_ucb(child)\n",
    "            if ucb > best_ucb:\n",
    "                best_child = child\n",
    "                best_ucb = ucb\n",
    "        return best_child\n",
    "    \n",
    "    def get_ucb(self, child):\n",
    "        if child.visit_count == 0:\n",
    "            q_value = 0\n",
    "        else:\n",
    "            q_value = 1 - ((child.value_sum / child.visit_count) + 1) / 2\n",
    "        return q_value + self.args['C'] * math.sqrt(self.visit_count / (child.visit_count + 1)) * child.prior\n",
    "    \n",
    "    def expand(self, policy):\n",
    "        for action, prob in enumerate(policy):\n",
    "            if prob > 0:\n",
    "                child_state = self.state.copy()\n",
    "                child_state = self.game.get_next_state(child_state, action, 1)\n",
    "                child_state = self.game.change_perspective(child_state, player=-1)\n",
    "\n",
    "                child = Node(self.game, self.args, child_state, self, action, prob)\n",
    "                self.children.append(child)\n",
    "        return child\n",
    "    \n",
    "    def backpropagate(self, value):\n",
    "        self.value_sum += value\n",
    "        self.visit_count += 1\n",
    "\n",
    "        value = self.game.get_opponent_value(value)\n",
    "        if self.parent is not None:\n",
    "            self.parent.backpropagate(value)\n",
    "\n",
    "class MCTS:\n",
    "    def __init__(self, game, args, model):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.model = model\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def search(self, state):\n",
    "        root = Node(self.game, self.args, state, visit_count=1)\n",
    "\n",
    "        policy, _ = self.model(\n",
    "            torch.tensor(self.game.get_encoded_state(state), device=self.model.device).unsqueeze(0)\n",
    "        )\n",
    "        policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
    "\n",
    "        policy = (1- self.args['dirichlet_epsiolon'] * policy + self.args['dirichlet_epsilon'] * np.random.dirichlet([self.args['dirichlet_alpha']] * self.game.action_size))\n",
    "\n",
    "        valid_moves = self.game.get_valid_moves(state)\n",
    "        policy *= valid_moves\n",
    "        policy /= np.sum(policy)\n",
    "        root.expand(policy)\n",
    "\n",
    "        for search in range(self.args['num_searches']):\n",
    "            node = root\n",
    "            while node.is_fully_expanded():\n",
    "                node = node.select()\n",
    "\n",
    "            value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
    "            value = self.game.get_opponent_value(value)\n",
    "\n",
    "            if not is_terminal:\n",
    "                policy, value = self.model(\n",
    "                    torch.tensor(self.game.get_encoded_state(node.state), device=self.model.device).unsqueeze(0)\n",
    "                )\n",
    "                policy = torch.softmax(policy, axis=1).squeeze(0).numpy()\n",
    "                valid_moves = self.game.get_valid_moves(node.state)\n",
    "                policy *= valid_moves\n",
    "                policy /= np.sum(policy)\n",
    "\n",
    "                value = value.item()\n",
    "\n",
    "                node = node.expand(policy)\n",
    "\n",
    "            node.backpropagate(value)\n",
    "\n",
    "        action_probs = np.zeros(self.game.action_size)\n",
    "        for child in root.children:\n",
    "            action_probs[child.action_taken] = child.visit_count\n",
    "        action_probs /= np.sum(action_probs)\n",
    "        \n",
    "        return action_probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiton AlphaZero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZero:\n",
    "    def __init__(self, model, optimizer, game, args):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.mcts = MCTS(game, args, model)\n",
    "    \n",
    "    def selfPlay(self):\n",
    "        memory = []\n",
    "        player = 1\n",
    "        state = self.game.get_initial_state()\n",
    "        \n",
    "        while True:\n",
    "            neutral_state = self.game.change_perspective(state, player)\n",
    "            action_probs = self.mcts.search(neutral_state)\n",
    "\n",
    "            memory.append((neutral_state, action_probs, player))\n",
    "\n",
    "            temp_action_probs = action_probs ** (1 / self.args['temperature'])\n",
    "            action = np.random.choice(self.game.action_size, p=temp_action_probs)\n",
    "\n",
    "            state = self.game.get_next_state(state, action, player)\n",
    "\n",
    "            value, is_terminal = self.game.get_value_and_terminated(state, action)\n",
    "\n",
    "            if is_terminal:\n",
    "                returnMemory = []\n",
    "                for hist_neutra_state, hist_action_probs, hist_player in memory:\n",
    "                    hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)\n",
    "                    returnMemory.append((\n",
    "                        self.game.get_encoded_state(hist_neutra_state),\n",
    "                        hist_action_probs,\n",
    "                        hist_outcome\n",
    "                    ))\n",
    "                return returnMemory\n",
    "            \n",
    "            player = self.game.get_opponent(player)\n",
    "\n",
    "    def train(self, memory):\n",
    "        random.shuffle(memory)\n",
    "        for batchIdx in range(0, len(memory), self.args['batch_size']):\n",
    "            sample = memory[batchIdx:min(len(memory) - 1, batchIdx+self.args['batch_size'])]\n",
    "            state, policy_targets, value_targets = zip(*sample)\n",
    "\n",
    "            state, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
    "\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=self.model.device)\n",
    "            policy_targets = torch.tensor(policy_targets, dtyte=torch.float32, device=self.model.device)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
    "\n",
    "            out_policy, out_value = self.model(state)\n",
    "\n",
    "            policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
    "            value_loss = F.mse_loss(out_value, value_targets)\n",
    "            loss = policy_loss + value_loss\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "    def learn(self):\n",
    "        for iteration in range(self.args['num_iterations']):\n",
    "            memory = []\n",
    "            \n",
    "            self.model.eval()\n",
    "            for SelPlay_iteration in range(self.args['num_selfPlay_iterations']):\n",
    "                memory += self.selfPlay()\n",
    "\n",
    "            self.model.train()\n",
    "            for epoch in range(self.args['num_epochs']):\n",
    "                self.train(memory)\n",
    "            \n",
    "            torch.save(self.model.state_dict(), f\"model_{iteration}_{self.game}.pt\")\n",
    "            torch.save(self.optimizer.state_dict(), f\"optimizer_{iteration}_{self.game}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTSParallel:\n",
    "    def __init__(self, game, args, model):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.model = model\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def search(self, states, spGames):\n",
    "        policy, _ = self.model(\n",
    "            torch.tensor(self.game.get_encoded_state(states), device=self.model.device)\n",
    "        )\n",
    "        policy = torch.softmax(policy, axis=1).cpu().numpy()\n",
    "\n",
    "        policy = (1 - self.args['dirichlet_epsiolon'] * policy + self.args['dirichlet_epsilon'] * np.random.dirichlet([self.args['dirichlet_alpha']] \\\n",
    "                    * self.game.action_size, size=policy.shape[0]))\n",
    "\n",
    "        for i, spg in enumerate(spGames):\n",
    "            spg_poliy = policy[i]\n",
    "            valid_moves = self.game.get_valid_moves(states[i])\n",
    "            spg_policy *= valid_moves\n",
    "            spg_policy /= np.sum(spg_policy)\n",
    "\n",
    "            spg.root = Node(self.game, self.args, states[i], visit_count=1)\n",
    "            spg.root.expand(spg_policy)\n",
    "\n",
    "        for search in range(self.args['num_searches']):\n",
    "            for spg in spGames:\n",
    "                spg.node = None\n",
    "                node = spg.root\n",
    "\n",
    "                while node.is_fully_expanded():\n",
    "                    node = node.select()\n",
    "\n",
    "                value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
    "                value = self.game.get_opponent_value(value)\n",
    "\n",
    "                if is_terminal:\n",
    "                    node.backpropagate(value)\n",
    "                \n",
    "                else:\n",
    "                    spg.node = node\n",
    "\n",
    "            expandable_spGames = [mappingIdx for mappingIdx in range(len(spGames)) if spGames[mappingIdx].node is not Node]\n",
    "\n",
    "            if len(expandable_spGames) > 0:\n",
    "                states = np.stack([spGames[mappingIdx].node.state for mappingIdx in expandable_spGames])\n",
    "\n",
    "                policy, value = self.model(\n",
    "                    torch.tensor(self.game.get_encoded_state(node.state), device=self.model.device)\n",
    "                )\n",
    "                policy = torch.softmax(policy, axis=1).numpy()\n",
    "\n",
    "                for i, mappingIdx in enumerate(expandable_spGames):\n",
    "                    node = spGames[mappingIdx].node\n",
    "                    spg_policy, spg_value = policy[i], value[i]\n",
    "                    valid_moves = self.game.get_valid_moves(node.state)\n",
    "                    spg_policy *= valid_moves\n",
    "                    spg_policypolicy /= np.sum(spg_policy)\n",
    "\n",
    "                    node = node.expand(spg_policy)\n",
    "                    node.backpropagate(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZeroParallel:\n",
    "    def __init__(self, model, optimizer, game, args):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.mcts = MCTSParallel(game, args, model)\n",
    "    \n",
    "    def selfPlay(self):\n",
    "        return_memory = []\n",
    "        player = 1\n",
    "        spGames = [SPG(self.game) for spg in range(self.args['num_parallel_games'])]\n",
    "\n",
    "        while len(spGames) > 0:\n",
    "            states = np.stack([spg.state for spg in spGames])\n",
    "\n",
    "            neutral_states = self.game.change_perspective(states, player)\n",
    "\n",
    "            self.mcts.search(neutral_states, spGames)\n",
    "\n",
    "            for i in range(len(spGames))[::-1]:\n",
    "                spg = spGames[i]\n",
    "\n",
    "                action_probs = np.zeros(self.game.action_size)\n",
    "                for child in spg.root.children:\n",
    "                    action_probs[child.action_taken] = child.visit_count\n",
    "                action_probs /= np.sum(action_probs)\n",
    "            \n",
    "                spg.memory.append((spg.root.state, action_probs, player))\n",
    "\n",
    "                temp_action_probs = action_probs ** (1 / self.args['temperature'])\n",
    "                action = np.random.choice(self.game.action_size, p=temp_action_probs)\n",
    "\n",
    "                spg.state = self.game.get_next_state(spg.state, action, player)\n",
    "\n",
    "                value, is_terminal = self.game.get_value_and_terminated(spg.state, action)\n",
    "\n",
    "                if is_terminal:\n",
    "                    for hist_neutra_state, hist_action_probs, hist_player in spg.memory:\n",
    "                        hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)\n",
    "                        return_memory.append((\n",
    "                            self.game.get_encoded_state(hist_neutra_state),\n",
    "                            hist_action_probs,\n",
    "                            hist_outcome\n",
    "                        ))\n",
    "                    del spGames[i]\n",
    "                \n",
    "            player = self.game.get_opponent(player)\n",
    "        \n",
    "        return return_memory\n",
    "            \n",
    "    def train(self, memory):\n",
    "        random.shuffle(memory)\n",
    "        for batchIdx in range(0, len(memory), self.args['batch_size']):\n",
    "            sample = memory[batchIdx:min(len(memory) - 1, batchIdx+self.args['batch_size'])]\n",
    "            state, policy_targets, value_targets = zip(*sample)\n",
    "\n",
    "            state, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
    "\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=self.model.device)\n",
    "            policy_targets = torch.tensor(policy_targets, dtyte=torch.float32, device=self.model.device)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
    "\n",
    "            out_policy, out_value = self.model(state)\n",
    "\n",
    "            policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
    "            value_loss = F.mse_loss(out_value, value_targets)\n",
    "            loss = policy_loss + value_loss\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "    def learn(self):\n",
    "        for iteration in range(self.args['num_iterations']):\n",
    "            memory = []\n",
    "            \n",
    "            self.model.eval()\n",
    "            for SelPlay_iteration in range(self.args['num_selfPlay_iterations'] // self.args['num_parallel_games']):\n",
    "                memory += self.selfPlay()\n",
    "\n",
    "            self.model.train()\n",
    "            for epoch in range(self.args['num_epochs']):\n",
    "                self.train(memory)\n",
    "            \n",
    "            torch.save(self.model.state_dict(), f\"model_{iteration}_{self.game}.pt\")\n",
    "            torch.save(self.optimizer.state_dict(), f\"optimizer_{iteration}_{self.game}.pt\")\n",
    "    \n",
    "class SPG:\n",
    "    def __init__(self, game):\n",
    "        self.state = game.get_initial_state()\n",
    "        self.memory = []\n",
    "        self.root = None\n",
    "        self.node = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TicTacToe Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:64] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "tensor() got an unexpected keyword argument 'dyte'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 22\u001b[0m\n\u001b[1;32m      9\u001b[0m args \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_searches\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m60\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdirichlet_alpha\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.3\u001b[39m\n\u001b[1;32m     19\u001b[0m }\n\u001b[1;32m     21\u001b[0m alphaZero \u001b[38;5;241m=\u001b[39m AlphaZero(model, optimizer, tictactoe, args)\n\u001b[0;32m---> 22\u001b[0m \u001b[43malphaZero\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 71\u001b[0m, in \u001b[0;36mAlphaZero.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m---> 71\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00miteration\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     74\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00miteration\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 47\u001b[0m, in \u001b[0;36mAlphaZero.train\u001b[0;34m(self, memory)\u001b[0m\n\u001b[1;32m     43\u001b[0m state, policy_targets, value_targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39msample)\n\u001b[1;32m     45\u001b[0m state, policy_targets, value_targets \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(state), np\u001b[38;5;241m.\u001b[39marray(policy_targets), np\u001b[38;5;241m.\u001b[39marray(value_targets)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 47\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdyte\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m policy_targets \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(policy_targets, dtyte\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     49\u001b[0m value_targets \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(value_targets, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mTypeError\u001b[0m: tensor() got an unexpected keyword argument 'dyte'"
     ]
    }
   ],
   "source": [
    "tictactoe = TicTacToe()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ResNet(tictactoe, 4, 64, device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 60,\n",
    "    'num_iterations': 3,\n",
    "    'num_selfPlay_iterations': 500,\n",
    "    'num_epochs': 4,\n",
    "    'batch_size': 64,\n",
    "    'temperature': 1.25,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'dirichlet_alpha': 0.3\n",
    "}\n",
    "\n",
    "alphaZero = AlphaZero(model, optimizer, tictactoe, args)\n",
    "alphaZero.learn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TicTacToe (AlphaZero plays against you now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "valid moves: [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "[[0. 1. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "[[ 0.  1. -1.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "valid moves: [0, 3, 4, 5, 6, 7, 8]\n",
      "[[ 0.  1. -1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "[[ 0.  1. -1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0. -1.]]\n",
      "valid moves: [0, 4, 5, 6, 7]\n",
      "[[ 0.  1. -1.]\n",
      " [ 1.  1.  0.]\n",
      " [ 0.  0. -1.]]\n",
      "[[ 0.  1. -1.]\n",
      " [ 1.  1. -1.]\n",
      " [ 0.  0. -1.]]\n",
      "-1 won\n"
     ]
    }
   ],
   "source": [
    "tictactoe = TicTacToe()\n",
    "player = 1\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 1000\n",
    "}\n",
    "\n",
    "model = ResNet(tictactoe, 4, 64)\n",
    "model.eval()\n",
    "\n",
    "mcts = MCTS(tictactoe, args, model)\n",
    "\n",
    "state = tictactoe.get_initial_state()\n",
    "\n",
    "while True:\n",
    "    print(state)\n",
    "\n",
    "    if player == 1:\n",
    "        valid_moves = tictactoe.get_valid_moves(state)\n",
    "        print(\"valid moves:\", [i for i in range(tictactoe.action_size) if valid_moves[i] == 1])\n",
    "        action = int(input(f\"{player}:\"))\n",
    "\n",
    "        if valid_moves[action] == 0:\n",
    "            print(\"action not valid\")\n",
    "            continue\n",
    "\n",
    "    else:\n",
    "        neutral_state = tictactoe.change_perspective(state, player)\n",
    "        mcts_probs = mcts.search(neutral_state)\n",
    "        action = np.argmax(mcts_probs)\n",
    "\n",
    "    state = tictactoe.get_next_state(state, action, player)\n",
    "\n",
    "    value, is_terminal = tictactoe.get_value_and_terminated(state, action)\n",
    "\n",
    "    if is_terminal:\n",
    "        print(state)\n",
    "        if value == 1:\n",
    "            print(player, \"won\")\n",
    "        else:\n",
    "            print(\"draw\")\n",
    "        break\n",
    "\n",
    "    player = tictactoe.get_opponent(player)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition ConnectFour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConnectFour:\n",
    "    def __init__(self):\n",
    "        self.height = 6\n",
    "        self.width = 7\n",
    "        self.action_size = self.width\n",
    "        self.in_a_row = 4\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"ConnectFour\"\n",
    "    \n",
    "    def get_initial_state(self):\n",
    "        return np.zeros((self.height, self.width))\n",
    "\n",
    "    def get_next_state(self, state, action, player):\n",
    "        row = np.max(np.where(state[:, action] == 0))\n",
    "        state[row, action] = player\n",
    "        return state\n",
    "    \n",
    "    def get_valid_moves(self, state):\n",
    "        return (state[0] == 0).astype(np.uint8)\n",
    "    \n",
    "    def check_win(self, state, action):\n",
    "        if action == None:\n",
    "            return False\n",
    "\n",
    "        row = np.min(np.where(state[:, action] != 0))\n",
    "        column = action\n",
    "        player = state[row][column]\n",
    "\n",
    "        def count(offset_row, offset_column):\n",
    "            for i in range(1, self.in_a_row):\n",
    "                r = row + offset_row * i\n",
    "                c = action + offset_column * i\n",
    "                if (\n",
    "                    r < 0\n",
    "                    or r >= self.height\n",
    "                    or c < 0\n",
    "                    or c >= self.width\n",
    "                    or state[r][c] != player\n",
    "                ):\n",
    "                    return i - 1\n",
    "            return self.in_a_row - 1\n",
    "        \n",
    "        return (\n",
    "            count (1, 0) >= self.in_a_row - 1\n",
    "            or (count(0, 1) + count(0, -1)) >= self.in_a_row - 1\n",
    "            or (count(1, 1) + count(-1, -1)) >= self.in_a_row - 1\n",
    "            or (count(1, -1) + count(-1, 1)) >= self.in_a_row - 1\n",
    "        )\n",
    "    \n",
    "    def get_value_and_terminated(self, state, action):\n",
    "        if self.check_win(state, action):\n",
    "            return 1, True\n",
    "        if np.sum(self.get_valid_moves(state)) == 0:\n",
    "            return 0, True\n",
    "        return 0, False\n",
    "    \n",
    "    def get_opponent(self, player):\n",
    "        return -player\n",
    "    \n",
    "    def get_opponent_value(self, value):\n",
    "        return -value\n",
    "    \n",
    "    def change_perspective(self, state, player):\n",
    "        return state * player\n",
    "    \n",
    "    def get_encoded_state(self, state):\n",
    "        encoded_state = np.stack(\n",
    "            (state == -1, state==0, state==1)\n",
    "        ).astype(np.float32)\n",
    "\n",
    "        if len(state.shape) == 3:\n",
    "            encoded_state = np.swapaxes(encoded_state, 0 , 1)\n",
    "\n",
    "        return encoded_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConnectFour Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "probabilities do not sum to 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 22\u001b[0m\n\u001b[1;32m      9\u001b[0m args \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_searches\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m600\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdirichlet_alpha\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.3\u001b[39m\n\u001b[1;32m     19\u001b[0m }\n\u001b[1;32m     21\u001b[0m alphaZero \u001b[38;5;241m=\u001b[39m AlphaZero(model, optimizer, game, args)\n\u001b[0;32m---> 22\u001b[0m \u001b[43malphaZero\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 68\u001b[0m, in \u001b[0;36mAlphaZero.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m SelPlay_iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_selfPlay_iterations\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m---> 68\u001b[0m     memory \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselfPlay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n",
      "Cell \u001b[0;32mIn[8], line 21\u001b[0m, in \u001b[0;36mAlphaZero.selfPlay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     18\u001b[0m memory\u001b[38;5;241m.\u001b[39mappend((neutral_state, action_probs, player))\n\u001b[1;32m     20\u001b[0m temp_action_probs \u001b[38;5;241m=\u001b[39m action_probs \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 21\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemp_action_probs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame\u001b[38;5;241m.\u001b[39mget_next_state(state, action, player)\n\u001b[1;32m     25\u001b[0m value, is_terminal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame\u001b[38;5;241m.\u001b[39mget_value_and_terminated(state, action)\n",
      "File \u001b[0;32mmtrand.pyx:958\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: probabilities do not sum to 1"
     ]
    }
   ],
   "source": [
    "game = ConnectFour()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ResNet(game, 9, 128, device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 600,\n",
    "    'num_iterations': 8,\n",
    "    'num_selfPlay_iterations': 500,\n",
    "    'num_parallel_games': 100,\n",
    "    'num_epochs': 4,\n",
    "    'batch_size': 128,\n",
    "    'temperature': 1.25,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'dirichlet_alpha': 0.3\n",
    "}\n",
    "\n",
    "alphaZero = AlphaZero(model, optimizer, game, args)\n",
    "alphaZero.learn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test ConnectFour (plays against you)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]]\n",
      "valid moves: [0, 1, 2, 3, 4, 5, 6]\n",
      "[[0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0. -1.  0.  0.  1.  0.]]\n",
      "valid moves: [0, 1, 2, 3, 4, 5, 6]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0. -1.  0.  0.  1.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.]\n",
      " [-1.  0. -1.  0.  0.  1.  0.]]\n",
      "valid moves: [0, 1, 2, 3, 4, 5, 6]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.]\n",
      " [-1.  0. -1.  1.  0.  1.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0. -1.  0.]\n",
      " [-1.  0. -1.  1.  0.  1.  0.]]\n",
      "valid moves: [0, 1, 2, 3, 4, 5, 6]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m valid_moves \u001b[38;5;241m=\u001b[39m game\u001b[38;5;241m.\u001b[39mget_valid_moves(state)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid moves:\u001b[39m\u001b[38;5;124m\"\u001b[39m, [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(game\u001b[38;5;241m.\u001b[39maction_size) \u001b[38;5;28;01mif\u001b[39;00m valid_moves[i] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m---> 24\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mplayer\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid_moves[action] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction not valid\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "game = ConnectFour()\n",
    "player = 1\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 100\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ResNet(game, 9, 128, device)\n",
    "model.eval()\n",
    "\n",
    "mcts = MCTS(game, args, model)\n",
    "\n",
    "state = game.get_initial_state()\n",
    "\n",
    "while True:\n",
    "    print(state)\n",
    "\n",
    "    if player == 1:\n",
    "        valid_moves = game.get_valid_moves(state)\n",
    "        print(\"valid moves:\", [i for i in range(game.action_size) if valid_moves[i] == 1])\n",
    "        action = int(input(f\"{player}:\"))\n",
    "\n",
    "        if valid_moves[action] == 0:\n",
    "            print(\"action not valid\")\n",
    "            continue\n",
    "\n",
    "    else:\n",
    "        neutral_state = game.change_perspective(state, player)\n",
    "        mcts_probs = mcts.search(neutral_state)\n",
    "        action = np.argmax(mcts_probs)\n",
    "\n",
    "    state = game.get_next_state(state, action, player)\n",
    "\n",
    "    value, is_terminal = game.get_value_and_terminated(state, action)\n",
    "\n",
    "    if is_terminal:\n",
    "        print(state)\n",
    "        if value == 1:\n",
    "            print(player, \"won\")\n",
    "        else:\n",
    "            print(\"draw\")\n",
    "        break\n",
    "\n",
    "    player = game.get_opponent(player)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
